# Aprendizaje Profundo (Cursos IA FIUBA 2020)

Ejercicios en Python del Curso Aprendizaje Profundo.

### Clase 01 (03/09/2020). Red neuronal sin vectorizar (numpy)

- [Notebook](clase_1/clase01.ipynb)

### Clase 02 (10/09/2020). Red neuronal vectorizada Mini Batch (numpy)

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> organización
- [Notebook versión no OO](clase_2/clase02.ipynb)
- [Notebook versión OO (keritas)](clase_2/clase02-keritas.ipynb)

### Clase 03 (17/09/2020). Red neuronal en Keras

- [Notebook](clase_3/clase03 - Modelo de Clase 2 en Keras.ipynb)
- [Notebook de clase (Collab/drive)](clase_3/01_Introducción_a_clasificación_con_Keras.ipynb)

### Clase 04 (24/09/2020)

- Teoría:
     - Curva ROC
   - Laboratorio:
     - [Detección de Anomalías con Keras](https://colab.research.google.com/drive/1KqFJQ1sYAdUwdO6X2QRaU9pwqkqz8p0D?usp=sharing)
   - Links:
      - [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
          - [Discrimination Threshold](https://www.scikit-yb.org/en/latest/api/classifier/threshold.html#discrimination-threshold)
          - [¿Qué es un Embedding?](https://www.youtube.com/watch?v=RkYuH_K7Fx4)

### Clase 05 (01/10/2020)

- Teoría (0-1:30):
     - KL Divergence 
          - [Ejemplo partiendo de comparación distribuciones](clase_5/Deep_Learning_Clase_5.ipynb)
          - [Optimizar DKl equivale a optimizar MV Cross Entropy](clase_5/pizarron/Deep Learning - Clase 5.pdf)
  - M.V. aplicada al modelo polinomial:
    - One Hot Encoding
    - Negative Sampling
- Laboratorio (>1:45hs):
    - [Embeddings con Keras](https://colab.research.google.com/drive/1RFnCHmbbZne40qBHVZp9t9-oo9qCE25s?usp=sharing)
    Links:
    - [Keras - Embedding Layers](https://www.kaggle.com/colinmorris/embedding-layers)

### Clase 06  (08/10/2020)

- Teoría (0-135hs)
     - Regularización. 
          - Bias vs. Varianza.
          - L1, L2, Dropout, Bagging (bootstrapping + aggregation)
          - Interpretación matemática de la regularización.
- Laboratorio (>1:40hs):
  - [Sequential vs Functional]()
  - [Autoencoders](clase_6/Autoencoders_con_Keras.ipynb)
- Links:

     - [BiasVariance](http://scott.fortmann-roe.com/docs/BiasVariance.html)
          -[Variational Autoencoder](https://www.siarez.com/projects/variational-autoencoder)
     - [Asymetric Variational Autoencoders](https://arxiv.org/pdf/1711.08352.pdf)

=======
[Notebook versión no OO](clase_2/clase02.ipynb)
[Notebook versión OO (keritas)](clase_2/clase02-keritas.ipynb)
<<<<<<< HEAD
=======
- [Notebook versión no OO](clase_2/clase02.ipynb)
- [Notebook versión OO (keritas)](clase_2/clase02-keritas.ipynb)
>>>>>>> deep learning flask para clase7 y haar para cv1

### Clase 03 (17/09/2020). Red neuronal en Keras

- [Notebook](clase_3/clase03 - Modelo de Clase 2 en Keras.ipynb)
- [Notebook de clase (Collab/drive)](clase_3/01_Introducción_a_clasificación_con_Keras.ipynb)

### Clase 04 (24/09/2020)

- Teoría:
     - Curva ROC
   - Laboratorio:
     - [Detección de Anomalías con Keras](https://colab.research.google.com/drive/1KqFJQ1sYAdUwdO6X2QRaU9pwqkqz8p0D?usp=sharing)
   - Links:
      - [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
          - [Discrimination Threshold](https://www.scikit-yb.org/en/latest/api/classifier/threshold.html#discrimination-threshold)
          - [¿Qué es un Embedding?](https://www.youtube.com/watch?v=RkYuH_K7Fx4)

### Clase 05 (01/10/2020)

- Teoría (0-1:30):
     - KL Divergence 
          - [Ejemplo partiendo de comparación distribuciones](clase_5/Deep_Learning_Clase_5.ipynb)
          - [Optimizar DKl equivale a optimizar MV Cross Entropy](clase_5/pizarron/Deep Learning - Clase 5.pdf)
  - M.V. aplicada al modelo polinomial:
    - One Hot Encoding
    - Negative Sampling
- Laboratorio (>1:45hs):
    - [Embeddings con Keras](https://colab.research.google.com/drive/1RFnCHmbbZne40qBHVZp9t9-oo9qCE25s?usp=sharing)
    Links:
    - [Keras - Embedding Layers](https://www.kaggle.com/colinmorris/embedding-layers)

<<<<<<< HEAD
=======

### Clase 03 (17/09/2020). Red neuronal en 

[Notebook](clase_3/clase03 - Modelo de Clase 2 en Keras.ipynb)
[Notebook de clase (Collab/drive)](clase_3/01_Introducción_a_clasificación_con_Keras.ipynb)

### Clase 04 (24/09/2020).

Teoría:
    - Curva ROC
Laboratorio:
    - [Detección de Anomalías con Keras](https://colab.research.google.com/drive/1KqFJQ1sYAdUwdO6X2QRaU9pwqkqz8p0D?usp=sharing)
Links:
    - [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
    - [Discrimination Threshold](https://www.scikit-yb.org/en/latest/api/classifier/threshold.html#discrimination-threshold)
    - [¿Qué es un Embedding?](https://www.youtube.com/watch?v=RkYuH_K7Fx4)

### Clase 05 (01/10/2020).

Teoría (0-1:30):
    - KL Divergence 
        - [Ejemplo partiendo de comparación distribuciones](clase_5/Deep_Learning_Clase_5.ipynb)
        - [Optimizar DKl equivale a optimizar MV Cross Entropy](clase_5/pizarron/Deep Learning - Clase 5.pdf)
    - M.V. aplicada al modelo polinomial:
        - One Hot Encoding
        - Negative Sampling
Laboratorio (>1:45hs:
    - [Embeddings con Keras](https://colab.research.google.com/drive/1RFnCHmbbZne40qBHVZp9t9-oo9qCE25s?usp=sharing)
Links:
    - [Keras - Embedding Layers](https://www.kaggle.com/colinmorris/embedding-layers)

>>>>>>> organización
### Clase 06  (08/10/2020).

Teoría (0-135hs)
    - Regularización. 
    - Bias vs. Varianza.
    - L1, L2, Dropout, Bagging (bootstrapping + aggregation)
    - Interpretación matemática de la regularización.
Laboratorio (>1:40hs):
    - [Sequential vs Functional]()
    - [Autoencoders](clase_6/Autoencoders_con_Keras.ipynb)
        
Links:
    - [BiasVariance](http://scott.fortmann-roe.com/docs/BiasVariance.html)
    -[Variational Autoencoder](https://www.siarez.com/projects/variational-autoencoder)
    - [Asymetric Variational Autoencoders](https://arxiv.org/pdf/1711.08352.pdf)
    - 
>>>>>>> organización
<<<<<<< HEAD
=======
### Clase 06  (08/10/2020)

- Teoría (0-135hs)
     - Regularización. 
          - Bias vs. Varianza.
          - L1, L2, Dropout, Bagging (bootstrapping + aggregation)
          - Interpretación matemática de la regularización.
- Laboratorio (>1:40hs):
  - [Sequential vs Functional]()
  - [Autoencoders](clase_6/Autoencoders_con_Keras.ipynb)
- Links:

     - [BiasVariance](http://scott.fortmann-roe.com/docs/BiasVariance.html)
          -[Variational Autoencoder](https://www.siarez.com/projects/variational-autoencoder)
     - [Asymetric Variational Autoencoders](https://arxiv.org/pdf/1711.08352.pdf)

>>>>>>> deep learning flask para clase7 y haar para cv1
=======
>>>>>>> organización
### Clase 07 (16/10/2020).

### Clase 08 (23/10/2020).

## Otros links y recursos de interés

- [Clarifiying exceptions and visualizing tensor operations](https://explained.ai/tensor-sensor/index.html)