{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Siguiendo:\n",
    "- \"Ensemble Learning for AI Developers. Learn Bagging, Stacking, and Boosting Methods with Use Cases\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1. Why Ensemble Techniques are Needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2. Mixing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Training a Decision Tree Using scikit-learn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(train_X, train_Y)\n",
    "\n",
    "print(tree.score(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Listing 2-2. Training Random Forest Using scikit-learn with Number of Decision Trees = 4\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, y,\n",
    "test_size = 0.1, random_state = 123)\n",
    "forest = RandomForestClassifier(n_estimators=8)\n",
    "forest = forest.fit(train_X, train_Y)\n",
    "\n",
    "print(forest.score(test_X, test_Y))\n",
    "\n",
    "rf_output = forest.predict(test_X)\n",
    "print(rf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Without Replacement (WOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples [[8, 1, 6, 7, 4], [4, 6, 5, 3, 8]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 2-3. Sampling Without Replacement in scikit-learn\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "# Random seed fixed so result could be replicated by Reader\n",
    "np.random.seed(123)\n",
    "#data to be sampled\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# Number of divisions needed\n",
    "num_divisions = 2\n",
    "list_of_data_divisions = []\n",
    "for x in range(0, num_divisions):\n",
    "   sample = resample(data, replace=False, n_samples=5)\n",
    "   list_of_data_divisions.append(sample)\n",
    "print('Samples', list_of_data_divisions)\n",
    "# Output: Samples [[8, 1, 6, 7, 4], [4, 6, 5, 3, 8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sammpling with Replacement (WR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples [[3, 3, 7, 2], [4, 7, 2, 1], [2, 1, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "#Listing 2-4. Sampling with Replacement in scikit-learn\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Random seed fixed so result could be replicated by Reader\n",
    "np.random.seed(123)\n",
    "# data to be sampled\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# Number of divisions needed\n",
    "num_divisions = 3\n",
    "list_of_data_divisions = []\n",
    "\n",
    "for x in range(0, num_divisions):\n",
    "   sample = resample(data, replace=True, n_samples=4)\n",
    "   list_of_data_divisions.append(sample)\n",
    "print(\"Samples\", list_of_data_divisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Listing 2-5. Bagging from Primitives\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data to be sampled\n",
    "n_samples = 100\n",
    "X,y = make_classification(n_samples=n_samples, n_features=4,\n",
    "n_informative=2, n_redundant=0, random_state=0, shuffle=False)\n",
    "\n",
    "#divide data into train and test set\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y,\n",
    "test_size = 0.1, random_state = 123)\n",
    "\n",
    "# Number of divisions needed\n",
    "num_divisions = 3\n",
    "list_of_data_divisions = []\n",
    "\n",
    "# Divide data into divisions\n",
    "for x in range(0, num_divisions):\n",
    "    X_train_sample, y_train_sample = resample(X_train, y_train, replace=True, n_samples=7)\n",
    "    sample = [X_train_sample, y_train_sample]\n",
    "    list_of_data_divisions.append(sample)\n",
    "    #print(list_of_data_divisions)\n",
    "\n",
    "# Learn a Classifier for each data divisions\n",
    "learners = []\n",
    "for data_division in list_of_data_divisions:\n",
    "    data_x = data_division[0]\n",
    "    data_y = data_division[1]\n",
    "    decision_tree = tree.DecisionTreeClassifier()\n",
    "    decision_tree.fit(data_x, data_y)\n",
    "    learners.append(decision_tree)\n",
    "\n",
    "# Combine output of all classifiers using voting\n",
    "predictions = []\n",
    "for i in range(len(y_test)):\n",
    "    counts = [0 for _ in range(num_divisions)]\n",
    "    for j , learner in enumerate(learners):\n",
    "        prediction = learner.predict([X_test[i]])\n",
    "        if prediction == 1:\n",
    "            counts[j] = counts[j] + 1\n",
    "    final_predictions = np.argmax(counts)\n",
    "    predictions.append(final_predictions)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Output: Accuracy: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "# Listing 2-6. Bagging scikit-learn\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=4,\n",
    "                          n_informative=2, n_redundant=0,\n",
    "                          random_state=0, shuffle=False)\n",
    "\n",
    "#divide data into train and test set\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y,\n",
    "test_size = 0.2, random_state = 123)\n",
    "clf = BaggingClassifier(base_estimator=SVC(),\n",
    "n_estimators=10, random_state=0).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "# Output: 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Folds Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n",
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [0 1] TEST: [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=2)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)\n",
    "\n",
    "# Output:\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified K-Folds Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      "TRAIN: [1 3] TEST: [0 2]\n",
      "TRAIN: [0 2] TEST: [1 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "skf.get_n_splits(X, y)\n",
    "print(skf)\n",
    "# Output:\n",
    "# StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "# Output:\n",
    "# TRAIN: [1 3] TEST: [0 2]\n",
    "# TRAIN: [0 2] TEST: [1 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3. Mixing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 3-1. Max Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 3-1. Max Voting Ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.3, stratify=y, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### k-Nearest Neighbors (k-NN)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "knn_gs.fit(X_train, y_train)\n",
    "knn_best = knn_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "params_rf = {'n_estimators': [50, 100, 200]}\n",
    "rf_gs = GridSearchCV(rf, params_rf, cv=5)\n",
    "rf_gs.fit(X_train, y_train)\n",
    "rf_best = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(random_state=123,\n",
    "solver='liblinear', penalty='l2', max_iter=5000)\n",
    "C = np.logspace(1, 4, 10)\n",
    "params_lr = dict(C=C)\n",
    "lr_gs = GridSearchCV(log_reg, params_lr, cv=5, verbose=0)\n",
    "lr_gs.fit(X_train, y_train)\n",
    "lr_best = lr_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_gs.score:  0.9239766081871345\n",
      "rf_gs.score:  0.9532163742690059\n",
      "log_reg.score:  0.9415204678362573\n",
      "ensemble.score:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# combine all three Voting Ensembles\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators=[\n",
    "    ('knn', knn_best), \n",
    "    ('rf', rf_best), \n",
    "    ('log_reg',lr_best)\n",
    "]\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "print(\"knn_gs.score: \", knn_best.score(X_test, y_test))\n",
    "print(\"rf_gs.score: \", rf_best.score(X_test, y_test))\n",
    "print(\"log_reg.score: \", lr_best.score(X_test, y_test))\n",
    "print(\"ensemble.score: \", ensemble.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaging/Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 3-2. Averaging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.3, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### k-Nearest Neighbors (k-NN)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "knn_gs.fit(X_train, y_train)\n",
    "knn_best = knn_gs.best_estimator_\n",
    "knn_gs_predictions = knn_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "params_rf = {'n_estimators': [50, 100, 200]}\n",
    "rf_gs = GridSearchCV(rf, params_rf, cv=5)\n",
    "rf_gs.fit(X_train, y_train)\n",
    "rf_best = rf_gs.best_estimator_\n",
    "rf_gs_predictions = rf_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(random_state=123,\n",
    "solver='liblinear', penalty='l2', max_iter=5000)\n",
    "C = np.logspace(1, 4, 10)\n",
    "params_lr = dict(C=C)\n",
    "lr_gs = GridSearchCV(log_reg, params_lr, cv=5, verbose=0)\n",
    "lr_gs.fit(X_train, y_train)\n",
    "lr_best = lr_gs.best_estimator_\n",
    "log_reg_predictions = lr_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_gs.score:  0.9239766081871345\n",
      "rf_gs.score:  0.9532163742690059\n",
      "log_reg.score:  0.9415204678362573\n",
      "ensemble.score:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# combine all three by averaging the Ensembles results\n",
    "#average_prediction = (log_reg_predictions + knn_gs_predictions + rf_gs_predictions)/3.0\n",
    "\n",
    "# Alternatively combine all through using VotingClassifier with voting='soft' parameter\n",
    "\n",
    "# combine all three Voting Ensembles\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators=[\n",
    "    ('knn', knn_best), \n",
    "    ('rf', rf_best), \n",
    "    ('log_reg', lr_best)\n",
    "]\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "print(\"knn_gs.score: \", knn_gs.score(X_test, y_test))\n",
    "print(\"rf_gs.score: \", rf_gs.score(X_test, y_test))\n",
    "print(\"log_reg.score: \", lr_gs.score(X_test, y_test))\n",
    "print(\"ensemble.score: \", ensemble.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Ensembles\n",
    "\n",
    "- Instead of relying on different models to make ensemble models, you use a good machine learning model and train this model using different hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 3-3. Hyperparameter Tuning Ensembles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.3, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_1 = RandomForestClassifier(random_state=0, n_estimators=10)\n",
    "rf_1.fit(X_train, y_train)\n",
    "\n",
    "rf_2 = RandomForestClassifier(random_state=0, n_estimators=50)\n",
    "rf_2.fit(X_train, y_train)\n",
    "\n",
    "rf_3 = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "rf_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_1.score:  0.935672514619883\n",
      "rf_2.score:  0.9473684210526315\n",
      "rf_3.score:  0.9532163742690059\n",
      "ensemble.score:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# combine all three Voting Ensembles\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators = [('rf_1', rf_1), ('rf_2', rf_2), ('rf_3', rf_3)]\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "print(\"rf_1.score: \", rf_1.score(X_test, y_test))\n",
    "print(\"rf_2.score: \", rf_2.score(X_test, y_test))\n",
    "print(\"rf_3.score: \", rf_3.score(X_test, y_test))\n",
    "print(\"ensemble.score: \", ensemble.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal Voting Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at Listing 3-4 where we have implemented horizontal voting ensembles using keras, tensorflow and scikit learn libraries.\n",
    "Ver págs 41-42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 3-4. Horizontal Voting Ensembles\n",
    "# Probar en Colab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot Ensembles\n",
    "\n",
    "- Snapshot ensembles are an extension of a horizontal voting ensemble. Instead of saving models after the minimum threshold, you modify the learning rate of the model itself.\n",
    "- When training a machine learning model, it is often desirable to start the initial higher learning and then slowly decrease the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4. Mixing Combinations\n",
    "\n",
    "- Introduce and explain boosting.\n",
    "- Examine how to implement boosting using scikit-learn.\n",
    "- Introduce and explain stacking.\n",
    "- Examine how to implement boosting using scikit-learn.\n",
    "- Look at other examples of mixing combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "- We start with a collection of learners. Each ML learner is trained on a particular subset of training objects. If a model learner has a weak performance, we could provide greater emphasis to that particular learner. This is known as boosting.\n",
    "- One of the simplest but most important of boosting techniques, AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666665\n"
     ]
    }
   ],
   "source": [
    "#Listing 4-1. AdaBoost Using scikit-learn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0 ).fit(X, y)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Listing 4-3. XGBoost Example on Breast Cancer Dataset Using scikit-learn and XGBoost Library\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# read in data\n",
    "iris = load_breast_cancer()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "# use DMatrix for xgbosot\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# set xgboost params\n",
    "param = {\n",
    "   'max_depth': 5,  # the maximum depth of each tree\n",
    "   'eta': 0.3,  # the training step for each iteration\n",
    "   'objective': 'multi:softprob',  # \u0007error evaluation formulticlass training\n",
    "   'num_class': 3}  # the number of classes that exist in this datset\n",
    "num_round = 200  # the number of training iterations\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)\n",
    "preds_rounded = np.argmax(preds, axis=1)\n",
    "print(accuracy_score(y_test, preds_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "- Key idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all learners in an ensemble, we train a model to perform this aggregation.\n",
    "- Base Learners, Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 4-4. Stacking Classifier Using scikit-learn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "estimators = [\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    (\"svr\", make_pipeline(StandardScaler(), LinearSVC(random_state=42))),\n",
    "]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=\n",
    "LogisticRegression())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3642619780615395"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 4-5. Stacking Regression Using scikit-learn\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "estimators = [(\"lr\", RidgeCV()), (\"svr\", LinearSVR(random_state=42))]\n",
    "reg = StackingRegressor(\n",
    "   estimators=estimators,\n",
    "   final_estimator=RandomForestRegressor(n_estimators=10, random_state=42),\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)\n",
    "reg.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5. Using Ensemble Learning Libraries\n",
    "\n",
    "- ML-Ensemble, a Python-based open source library that wraps scikit ensemble classes to offer a high-level API.\n",
    "- Scale XGBoost via Dask, a flexible library for parallel computing in Python. Dask and XGBoost can work together to train gradient-boosted trees in parallel.\n",
    "- Learn boosting using Microsoft LightGBM.\n",
    "- Introduce AdaNet, a lightweight TensorFlow-based framework for learning neural network architecture, but is also used for learning to ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-Ensemble\n",
    "\n",
    "also: mlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fdca0700310>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/mlens/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fdca0700880>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/mlens/\u001b[0m\n",
      "Collecting mlens\n",
      "  Downloading mlens-0.2.3-py2.py3-none-any.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 561 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17 in /opt/conda/lib/python3.8/site-packages (from mlens) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.8/site-packages (from mlens) (1.19.2)\n",
      "Installing collected packages: mlens\n",
      "Successfully installed mlens-0.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install mlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit data:\n",
      "                                   score-m  score-s  ft-m  ft-s  pt-m  pt-s\n",
      "layer-1  svc                          0.01     0.00  0.14  0.02  0.02  0.00\n",
      "layer-1  randomforestclassifier       0.00     0.00  0.57  0.01  0.04  0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---Data setup----\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_moons\n",
    "seed = 42\n",
    "\n",
    "X, y = make_moons(n_samples=10000,noise=0.4, random_state=seed)\n",
    "\n",
    "# --- 1. Initialize ---\n",
    "from mlens.ensemble import SuperLearner\n",
    "ensemble = SuperLearner(scorer=accuracy_score, random_state=seed)\n",
    "\n",
    "# --- 2. Build the first layer ---\n",
    "ensemble.add([RandomForestClassifier(random_state=seed),\n",
    "SVC(random_state=seed)])\n",
    "\n",
    "# --- 3. Attach the final meta learner\n",
    "ensemble.add_meta(LogisticRegression(solver=\"sag\", max_iter=2000))\n",
    "\n",
    "# --- Train ---\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# --- Predict ---\n",
    "preds = ensemble.predict(X_test)\n",
    "\n",
    "print(\"Fit data:\\n%r\" % ensemble.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=2,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
       "   random_state=7270, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, folds=2, raise_on_ex...d97670>)],\n",
       "   n_jobs=-1, name='group-12', raise_on_exception=True, transformers=[])],\n",
       "   verbose=1)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=42, sample_size=20,\n",
       "       scorer=<function accuracy_score at 0x7fd20bd97670>, shuffle=False,\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = SuperLearner(scorer=accuracy_score, random_state=seed, verbose=2)\n",
    "\n",
    "# Build the first layer\n",
    "ensemble.add([RandomForestClassifier(random_state=seed),LogisticRegression(random_state=seed)])\n",
    "\n",
    "# Build the 2nd layer\n",
    "ensemble.add([LogisticRegression(random_state=seed),\n",
    "SVC(random_state=seed)])\n",
    "\n",
    "# Attach the final meta estimator\n",
    "ensemble.add_meta(SVC(random_state=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.model_selection import Evaluator\n",
    "from scipy.stats import randint\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "The mlens preprocessing feature helps you compare the models across a set of preprocessing pipelines. It does this via a class that acts as a transformer, allowing you to use lower or incoming layers as a “preprocessing” step, so that you need only evaluate the metalearners iteratively. Let’s look at the code to understand it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_score-m</th>\n",
       "      <th>test_score-s</th>\n",
       "      <th>train_score-m</th>\n",
       "      <th>train_score-s</th>\n",
       "      <th>fit_time-m</th>\n",
       "      <th>fit_time-s</th>\n",
       "      <th>pred_time-m</th>\n",
       "      <th>pred_time-s</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class.rf</th>\n",
       "      <td>0.8628</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.8606</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5.041326</td>\n",
       "      <td>0.252750</td>\n",
       "      <td>0.452895</td>\n",
       "      <td>0.063201</td>\n",
       "      <td>{'max_depth': 8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class.svc</th>\n",
       "      <td>0.8628</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.8606</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1.717284</td>\n",
       "      <td>0.038328</td>\n",
       "      <td>0.079342</td>\n",
       "      <td>0.044751</td>\n",
       "      <td>{'C': 3.745401188473625}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proba.rf</th>\n",
       "      <td>0.8600</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>4.437818</td>\n",
       "      <td>0.012086</td>\n",
       "      <td>0.565088</td>\n",
       "      <td>0.073240</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 0.97535715320...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proba.svc</th>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.8606</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.122270</td>\n",
       "      <td>0.137853</td>\n",
       "      <td>0.078443</td>\n",
       "      <td>{'C': 3.745401188473625}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           test_score-m  test_score-s  train_score-m  train_score-s  \\\n",
       "class.rf         0.8628        0.0016         0.8606         0.0002   \n",
       "class.svc        0.8628        0.0016         0.8606         0.0002   \n",
       "proba.rf         0.8600        0.0002         0.8734         0.0046   \n",
       "proba.svc        0.8625        0.0007         0.8606         0.0010   \n",
       "\n",
       "           fit_time-m  fit_time-s  pred_time-m  pred_time-s  \\\n",
       "class.rf     5.041326    0.252750     0.452895     0.063201   \n",
       "class.svc    1.717284    0.038328     0.079342     0.044751   \n",
       "proba.rf     4.437818    0.012086     0.565088     0.073240   \n",
       "proba.svc    0.866501    0.122270     0.137853     0.078443   \n",
       "\n",
       "                                                      params  \n",
       "class.rf                                    {'max_depth': 8}  \n",
       "class.svc                           {'C': 3.745401188473625}  \n",
       "proba.rf   {'max_depth': 5, 'max_features': 0.97535715320...  \n",
       "proba.svc                           {'C': 3.745401188473625}  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlens.model_selection import Evaluator\n",
    "from mlens.ensemble import SequentialEnsemble #--1\n",
    "from mlens.metrics import make_scorer\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "base_learners = [RandomForestClassifier(random_state=seed), SVC(probability=True)] #--2\n",
    "\n",
    "proba_transformer = SequentialEnsemble(model_selection=True, random_state=seed) \\\n",
    "                        .add('blend', base_learners, proba=True) #--3\n",
    "\n",
    "class_transformer = SequentialEnsemble(model_selection=True, random_state=seed) \\\n",
    "                        .add('blend', base_learners, proba=False) #--4\n",
    "\n",
    "preprocessing = {\n",
    "    'proba': [('layer-1', proba_transformer)],\n",
    "    'class': [('layer-1', class_transformer)]\n",
    "} #--5\n",
    "\n",
    "meta_learners = [\n",
    "    SVC(random_state=seed), \n",
    "    ('rf',RandomForestClassifier(random_state=seed))\n",
    "] #--6\n",
    "        \n",
    "params = {\n",
    "    'svc': {'C': uniform(0, 10)},\n",
    "    'class.rf': {'max_depth': randint(2, 10)},\n",
    "    'proba.rf': {'max_depth': randint(2, 10), 'max_features': uniform(0.5, 0.5)}\n",
    "} #--7\n",
    "\n",
    "scorer = make_scorer(accuracy_score) #--8\n",
    "evaluator = Evaluator(scorer=scorer, random_state=seed, cv=2) #--9\n",
    "evaluator.fit(X, y, meta_learners, params,\n",
    "preprocessing=preprocessing, n_iter=2)#--10\n",
    "\n",
    "from pandas import DataFrame\n",
    "df = DataFrame(evaluator.results) #--11\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to turn off! parameter -> model_selection=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask c/ XGBoost (y Numpy, Pandas, etc.)\n",
    "\n",
    "Fuera de scope ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "Fuera de scope ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaNet\n",
    "\n",
    "Ejemplos en Google Colab (https://github.com/tensorflow/adanet). The notebooks are well annotated and provide ready-to-use boilerplate code to use in your ML tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6. Tips and Best Practices\n",
    "\n",
    "- Feature selection using a random forest model. It should not come as a surprise that feature selection and feature relevance benefits from the performance and interpretation of machine learning algorithms.\n",
    "- Feature transformations with ensembles of trees.\n",
    "- Building a preprocessing pipeline for a random forest regressor.\n",
    "- Isolation forests, an efficient algorithm for outlier detection, especially in high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "sepal length (cm) 0.09906957842524829\n",
      "sepal width (cm) 0.03880497890715764\n",
      "petal length (cm) 0.4152569088750478\n",
      "petal width (cm) 0.4468685337925464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris() # -1\n",
    "feature_list = iris.feature_names # -2\n",
    "print(feature_list)\n",
    "['sepal length (cm)',\n",
    "'sepal width (cm)',\n",
    "'petal length (cm)',\n",
    "'petal width (cm)']\n",
    "\n",
    "X = iris.data # -3\n",
    "y = iris.target # -4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42) # -5\n",
    "rf_clf = RandomForestClassifier(n_estimators=10000, random_state=42, n_jobs=-1) # -6\n",
    "rf_clf.fit(X_train, y_train) # -7\n",
    "\n",
    "for name, score in zip(iris[\"feature_names\"], rf_clf.feature_importances_):\n",
    "     print(name, score) # -8\n",
    "\n",
    "y_pred = clf.predict(X_test) # -9\n",
    "accuracy_score(y_test, y_pred) # -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel \n",
    "\n",
    "sfm = SelectFromModel(rf_clf, threshold=0.15) # - 11\n",
    "sfm.fit(X_train, y_train) # -12\n",
    "X_important_train = sfm.transform(X_train) # -13\n",
    "X_important_test = sfm.transform(X_test)\n",
    "rf_clf_important = RandomForestClassifier(n_estimators=500, random_state=0, n_jobs=-1) # -14\n",
    "\n",
    "\n",
    "rf_clf_important.fit(X_important_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_important_pred = rf_clf_important.predict(X_important_test) # - 15\n",
    "accuracy_score(y_test, y_important_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformations with Ensembles of Trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
